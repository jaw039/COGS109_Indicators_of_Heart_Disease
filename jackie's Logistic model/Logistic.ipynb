{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# Logistic Regression with Subset Selection\n",
        "\n",
        "## Variant 1: Feature Subset Selection for Heart Attack Prediction\n",
        "\n",
        "**Goal**: Find the smallest useful set of predictors for heart attack prediction\n",
        "\n",
        "**Methods**:\n",
        "- Best subset selection\n",
        "- Forward stepwise selection\n",
        "- Backward stepwise selection\n",
        "\n",
        "**Selection Criteria**: AIC/BIC optimization\n",
        "\n",
        "**Validation**: 10-fold cross-validation for error/AUC confirmation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical modeling\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.discrete.discrete_model import Logit\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Data loading\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /Users/jackiewang/.cache/kagglehub/datasets/kamilpytlak/personal-key-indicators-of-heart-disease/versions/6\n",
            "Dataset shape: (246022, 40)\n",
            "Target variable distribution:\n",
            "HadHeartAttack\n",
            "No     232587\n",
            "Yes     13435\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Download and load the dataset\n",
        "path = kagglehub.dataset_download(\"kamilpytlak/personal-key-indicators-of-heart-disease\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = os.path.join(path, '2022', 'heart_2022_no_nans.csv')\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Target variable distribution:\")\n",
        "print(df['HadHeartAttack'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total candidate features: 18\n",
            "\n",
            "Features by category:\n",
            "  demographic: ['Sex', 'AgeCategory', 'RaceEthnicityCategory']\n",
            "  health_conditions: ['HadDiabetes', 'HadStroke', 'GeneralHealth', 'HadAsthma', 'HadKidneyDisease', 'HadCOPD', 'HadArthritis', 'HadDepressiveDisorder']\n",
            "  lifestyle: ['SmokerStatus', 'AlcoholDrinkers', 'PhysicalActivities']\n",
            "  health_metrics: ['BMI', 'PhysicalHealthDays', 'MentalHealthDays', 'SleepHours']\n",
            "\n",
            "Working dataset shape: (246022, 19)\n",
            "Missing values: 0\n"
          ]
        }
      ],
      "source": [
        "# Define candidate features for selection\n",
        "# Based on research questions: diabetes, stroke, general health + demographics + lifestyle\n",
        "\n",
        "candidate_features = {\n",
        "    'demographic': ['Sex', 'AgeCategory', 'RaceEthnicityCategory'],\n",
        "    'health_conditions': ['HadDiabetes', 'HadStroke', 'GeneralHealth', 'HadAsthma', \n",
        "                          'HadKidneyDisease', 'HadCOPD', 'HadArthritis', 'HadDepressiveDisorder'],\n",
        "    'lifestyle': ['SmokerStatus', 'AlcoholDrinkers', 'PhysicalActivities'],\n",
        "    'health_metrics': ['BMI', 'PhysicalHealthDays', 'MentalHealthDays', 'SleepHours']\n",
        "}\n",
        "\n",
        "# Flatten all candidate features\n",
        "all_features = []\n",
        "for category, features in candidate_features.items():\n",
        "    all_features.extend(features)\n",
        "\n",
        "print(f\"Total candidate features: {len(all_features)}\")\n",
        "print(\"\\nFeatures by category:\")\n",
        "for category, features in candidate_features.items():\n",
        "    print(f\"  {category}: {features}\")\n",
        "\n",
        "# Create working dataset\n",
        "df_model = df[all_features + ['HadHeartAttack']].copy()\n",
        "print(f\"\\nWorking dataset shape: {df_model.shape}\")\n",
        "print(f\"Missing values: {df_model.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DATA PREPROCESSING ===\n",
            "Categorical columns to encode: ['Sex', 'AgeCategory', 'RaceEthnicityCategory', 'HadDiabetes', 'HadStroke', 'GeneralHealth', 'HadAsthma', 'HadKidneyDisease', 'HadCOPD', 'HadArthritis', 'HadDepressiveDisorder', 'SmokerStatus', 'AlcoholDrinkers', 'PhysicalActivities']\n",
            "Encoded dataset shape: (246022, 40)\n",
            "Number of features: 39\n",
            "\n",
            "Final feature set: 39 features\n",
            "Target distribution: {0: 232587, 1: 13435}\n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing for modeling\n",
        "print(\"=== DATA PREPROCESSING ===\")\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_cols = df_model.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_cols.remove('HadHeartAttack')  # Remove target variable\n",
        "\n",
        "print(f\"Categorical columns to encode: {categorical_cols}\")\n",
        "\n",
        "# Create dummy variables\n",
        "df_encoded = pd.get_dummies(df_model, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Convert target to binary\n",
        "df_encoded['y'] = (df_encoded['HadHeartAttack'] == 'Yes').astype(int)\n",
        "df_encoded = df_encoded.drop('HadHeartAttack', axis=1)\n",
        "\n",
        "print(f\"Encoded dataset shape: {df_encoded.shape}\")\n",
        "print(f\"Number of features: {df_encoded.shape[1] - 1}\")\n",
        "\n",
        "# Split into features and target\n",
        "X = df_encoded.drop('y', axis=1)\n",
        "y = df_encoded['y']\n",
        "\n",
        "print(f\"\\nFinal feature set: {X.shape[1]} features\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: (196817, 39)\n",
            "Test set: (49205, 39)\n",
            "Scaled features: ['BMI', 'PhysicalHealthDays', 'MentalHealthDays', 'SleepHours']\n"
          ]
        }
      ],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale numerical features\n",
        "numerical_features = ['BMI', 'PhysicalHealthDays', 'MentalHealthDays', 'SleepHours']\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "# Scale only numerical columns that exist in the dataset\n",
        "num_cols_to_scale = [col for col in numerical_features if col in X_train.columns]\n",
        "if num_cols_to_scale:\n",
        "    X_train_scaled[num_cols_to_scale] = scaler.fit_transform(X_train[num_cols_to_scale])\n",
        "    X_test_scaled[num_cols_to_scale] = scaler.transform(X_test[num_cols_to_scale])\n",
        "\n",
        "print(f\"Training set: {X_train_scaled.shape}\")\n",
        "print(f\"Test set: {X_test_scaled.shape}\")\n",
        "print(f\"Scaled features: {num_cols_to_scale}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Method 1: Best Subset Selection\n",
        "\n",
        "**Approach**: Evaluate all possible combinations of features up to a maximum size\n",
        "**Criteria**: Select model with lowest AIC/BIC\n",
        "**Computational Note**: Limited to reasonable subset sizes due to 2^p complexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== BEST SUBSET SELECTION ===\n",
            "Starting best subset selection with 39 features\n",
            "Evaluating subsets up to size 8\n",
            "\n",
            "Evaluating subsets of size 1...\n",
            "  Testing 39 combinations\n",
            "  Best AIC (1 features): 80757.463, Features: ['PhysicalHealthDays']\n",
            "  Best BIC (1 features): 80777.843, Features: ['PhysicalHealthDays']\n",
            "\n",
            "Evaluating subsets of size 2...\n",
            "  Testing 741 combinations\n",
            "  Best AIC (2 features): 80718.389, Features: ['BMI', 'PhysicalHealthDays']\n",
            "  Best BIC (2 features): 80748.959, Features: ['BMI', 'PhysicalHealthDays']\n",
            "\n",
            "Evaluating subsets of size 3...\n",
            "  Testing 9139 combinations\n",
            "    Processed 1000/9139 combinations\n",
            "    Processed 2000/9139 combinations\n",
            "    Processed 3000/9139 combinations\n",
            "    Processed 4000/9139 combinations\n",
            "    Processed 5000/9139 combinations\n",
            "    Processed 6000/9139 combinations\n",
            "    Processed 7000/9139 combinations\n",
            "    Processed 8000/9139 combinations\n",
            "    Processed 9000/9139 combinations\n",
            "  Best AIC (3 features): 80681.352, Features: ['BMI', 'PhysicalHealthDays', 'MentalHealthDays']\n",
            "  Best BIC (3 features): 80722.112, Features: ['BMI', 'PhysicalHealthDays', 'MentalHealthDays']\n",
            "\n",
            "Evaluating subsets of size 4...\n",
            "  Testing 82251 combinations\n",
            "    Processed 1000/82251 combinations\n",
            "    Processed 2000/82251 combinations\n",
            "    Processed 3000/82251 combinations\n",
            "    Processed 4000/82251 combinations\n",
            "    Processed 5000/82251 combinations\n",
            "    Processed 6000/82251 combinations\n",
            "    Processed 7000/82251 combinations\n",
            "    Processed 8000/82251 combinations\n",
            "    Processed 9000/82251 combinations\n",
            "    Processed 10000/82251 combinations\n",
            "    Processed 11000/82251 combinations\n",
            "    Processed 12000/82251 combinations\n",
            "    Processed 13000/82251 combinations\n",
            "    Processed 14000/82251 combinations\n",
            "    Processed 15000/82251 combinations\n",
            "    Processed 16000/82251 combinations\n",
            "    Processed 17000/82251 combinations\n",
            "    Processed 18000/82251 combinations\n",
            "    Processed 19000/82251 combinations\n",
            "    Processed 20000/82251 combinations\n",
            "    Processed 21000/82251 combinations\n",
            "    Processed 22000/82251 combinations\n",
            "    Processed 23000/82251 combinations\n",
            "    Processed 24000/82251 combinations\n",
            "    Processed 25000/82251 combinations\n",
            "    Processed 26000/82251 combinations\n",
            "    Processed 27000/82251 combinations\n",
            "    Processed 28000/82251 combinations\n",
            "    Processed 29000/82251 combinations\n",
            "    Processed 30000/82251 combinations\n",
            "    Processed 31000/82251 combinations\n",
            "    Processed 32000/82251 combinations\n",
            "    Processed 33000/82251 combinations\n",
            "    Processed 34000/82251 combinations\n",
            "    Processed 35000/82251 combinations\n",
            "    Processed 36000/82251 combinations\n",
            "    Processed 37000/82251 combinations\n",
            "    Processed 38000/82251 combinations\n",
            "    Processed 39000/82251 combinations\n",
            "    Processed 40000/82251 combinations\n",
            "    Processed 41000/82251 combinations\n",
            "    Processed 42000/82251 combinations\n",
            "    Processed 43000/82251 combinations\n",
            "    Processed 44000/82251 combinations\n",
            "    Processed 45000/82251 combinations\n",
            "    Processed 46000/82251 combinations\n",
            "    Processed 47000/82251 combinations\n",
            "    Processed 48000/82251 combinations\n",
            "    Processed 49000/82251 combinations\n",
            "    Processed 50000/82251 combinations\n",
            "    Processed 51000/82251 combinations\n",
            "    Processed 52000/82251 combinations\n",
            "    Processed 53000/82251 combinations\n",
            "    Processed 54000/82251 combinations\n",
            "    Processed 55000/82251 combinations\n",
            "    Processed 56000/82251 combinations\n",
            "    Processed 57000/82251 combinations\n",
            "    Processed 58000/82251 combinations\n",
            "    Processed 59000/82251 combinations\n",
            "    Processed 60000/82251 combinations\n",
            "    Processed 61000/82251 combinations\n",
            "    Processed 62000/82251 combinations\n",
            "    Processed 63000/82251 combinations\n",
            "    Processed 64000/82251 combinations\n",
            "    Processed 65000/82251 combinations\n",
            "    Processed 66000/82251 combinations\n",
            "    Processed 67000/82251 combinations\n",
            "    Processed 68000/82251 combinations\n",
            "    Processed 69000/82251 combinations\n",
            "    Processed 70000/82251 combinations\n",
            "    Processed 71000/82251 combinations\n",
            "    Processed 72000/82251 combinations\n",
            "    Processed 73000/82251 combinations\n",
            "    Processed 74000/82251 combinations\n",
            "    Processed 75000/82251 combinations\n",
            "    Processed 76000/82251 combinations\n",
            "    Processed 77000/82251 combinations\n",
            "    Processed 78000/82251 combinations\n",
            "    Processed 79000/82251 combinations\n",
            "    Processed 80000/82251 combinations\n",
            "    Processed 81000/82251 combinations\n",
            "    Processed 82000/82251 combinations\n",
            "  Best AIC (4 features): 80666.625, Features: ['BMI', 'PhysicalHealthDays', 'MentalHealthDays', 'SleepHours']\n",
            "  Best BIC (4 features): 80717.576, Features: ['BMI', 'PhysicalHealthDays', 'MentalHealthDays', 'SleepHours']\n",
            "\n",
            "Evaluating subsets of size 5...\n",
            "  Testing 575757 combinations\n",
            "    Processed 1000/575757 combinations\n",
            "    Processed 2000/575757 combinations\n",
            "    Processed 3000/575757 combinations\n",
            "    Processed 4000/575757 combinations\n",
            "    Processed 5000/575757 combinations\n",
            "    Processed 6000/575757 combinations\n",
            "    Processed 7000/575757 combinations\n",
            "    Processed 8000/575757 combinations\n",
            "    Processed 9000/575757 combinations\n",
            "    Processed 10000/575757 combinations\n",
            "    Processed 11000/575757 combinations\n",
            "    Processed 12000/575757 combinations\n",
            "    Processed 13000/575757 combinations\n",
            "    Processed 14000/575757 combinations\n",
            "    Processed 15000/575757 combinations\n",
            "    Processed 16000/575757 combinations\n",
            "    Processed 17000/575757 combinations\n",
            "    Processed 18000/575757 combinations\n",
            "    Processed 19000/575757 combinations\n",
            "    Processed 20000/575757 combinations\n",
            "    Processed 21000/575757 combinations\n",
            "    Processed 22000/575757 combinations\n",
            "    Processed 23000/575757 combinations\n",
            "    Processed 24000/575757 combinations\n",
            "    Processed 25000/575757 combinations\n",
            "    Processed 26000/575757 combinations\n",
            "    Processed 27000/575757 combinations\n",
            "    Processed 28000/575757 combinations\n",
            "    Processed 29000/575757 combinations\n",
            "    Processed 30000/575757 combinations\n",
            "    Processed 31000/575757 combinations\n",
            "    Processed 32000/575757 combinations\n",
            "    Processed 33000/575757 combinations\n",
            "    Processed 34000/575757 combinations\n",
            "    Processed 35000/575757 combinations\n",
            "    Processed 36000/575757 combinations\n",
            "    Processed 37000/575757 combinations\n",
            "    Processed 38000/575757 combinations\n",
            "    Processed 39000/575757 combinations\n",
            "    Processed 40000/575757 combinations\n",
            "    Processed 41000/575757 combinations\n",
            "    Processed 42000/575757 combinations\n",
            "    Processed 43000/575757 combinations\n",
            "    Processed 44000/575757 combinations\n",
            "    Processed 45000/575757 combinations\n",
            "    Processed 46000/575757 combinations\n",
            "    Processed 47000/575757 combinations\n",
            "    Processed 48000/575757 combinations\n",
            "    Processed 49000/575757 combinations\n",
            "    Processed 50000/575757 combinations\n",
            "    Processed 51000/575757 combinations\n",
            "    Processed 52000/575757 combinations\n",
            "    Processed 53000/575757 combinations\n",
            "    Processed 54000/575757 combinations\n",
            "    Processed 55000/575757 combinations\n",
            "    Processed 56000/575757 combinations\n",
            "    Processed 57000/575757 combinations\n",
            "    Processed 58000/575757 combinations\n",
            "    Processed 59000/575757 combinations\n",
            "    Processed 60000/575757 combinations\n",
            "    Processed 61000/575757 combinations\n",
            "    Processed 62000/575757 combinations\n",
            "    Processed 63000/575757 combinations\n",
            "    Processed 64000/575757 combinations\n",
            "    Processed 65000/575757 combinations\n",
            "    Processed 66000/575757 combinations\n",
            "    Processed 67000/575757 combinations\n",
            "    Processed 68000/575757 combinations\n",
            "    Processed 69000/575757 combinations\n",
            "    Processed 70000/575757 combinations\n",
            "    Processed 71000/575757 combinations\n",
            "    Processed 72000/575757 combinations\n",
            "    Processed 73000/575757 combinations\n",
            "    Processed 74000/575757 combinations\n",
            "    Processed 75000/575757 combinations\n",
            "    Processed 76000/575757 combinations\n",
            "    Processed 77000/575757 combinations\n",
            "    Processed 78000/575757 combinations\n",
            "    Processed 79000/575757 combinations\n",
            "    Processed 80000/575757 combinations\n",
            "    Processed 81000/575757 combinations\n",
            "    Processed 82000/575757 combinations\n",
            "    Processed 83000/575757 combinations\n",
            "    Processed 84000/575757 combinations\n",
            "    Processed 85000/575757 combinations\n",
            "    Processed 86000/575757 combinations\n",
            "    Processed 87000/575757 combinations\n",
            "    Processed 88000/575757 combinations\n",
            "    Processed 89000/575757 combinations\n",
            "    Processed 90000/575757 combinations\n",
            "    Processed 91000/575757 combinations\n",
            "    Processed 92000/575757 combinations\n",
            "    Processed 93000/575757 combinations\n",
            "    Processed 94000/575757 combinations\n",
            "    Processed 95000/575757 combinations\n",
            "    Processed 96000/575757 combinations\n",
            "    Processed 97000/575757 combinations\n",
            "    Processed 98000/575757 combinations\n",
            "    Processed 99000/575757 combinations\n",
            "    Processed 100000/575757 combinations\n",
            "    Processed 101000/575757 combinations\n",
            "    Processed 102000/575757 combinations\n",
            "    Processed 103000/575757 combinations\n",
            "    Processed 104000/575757 combinations\n",
            "    Processed 105000/575757 combinations\n",
            "    Processed 106000/575757 combinations\n",
            "    Processed 107000/575757 combinations\n",
            "    Processed 108000/575757 combinations\n",
            "    Processed 109000/575757 combinations\n",
            "    Processed 110000/575757 combinations\n",
            "    Processed 111000/575757 combinations\n",
            "    Processed 112000/575757 combinations\n",
            "    Processed 113000/575757 combinations\n",
            "    Processed 114000/575757 combinations\n",
            "    Processed 115000/575757 combinations\n",
            "    Processed 116000/575757 combinations\n",
            "    Processed 117000/575757 combinations\n",
            "    Processed 118000/575757 combinations\n",
            "    Processed 119000/575757 combinations\n",
            "    Processed 120000/575757 combinations\n",
            "    Processed 121000/575757 combinations\n",
            "    Processed 122000/575757 combinations\n",
            "    Processed 123000/575757 combinations\n",
            "    Processed 124000/575757 combinations\n",
            "    Processed 125000/575757 combinations\n",
            "    Processed 126000/575757 combinations\n",
            "    Processed 127000/575757 combinations\n",
            "    Processed 128000/575757 combinations\n",
            "    Processed 129000/575757 combinations\n",
            "    Processed 130000/575757 combinations\n",
            "    Processed 131000/575757 combinations\n",
            "    Processed 132000/575757 combinations\n",
            "    Processed 133000/575757 combinations\n",
            "    Processed 134000/575757 combinations\n",
            "    Processed 135000/575757 combinations\n",
            "    Processed 136000/575757 combinations\n",
            "    Processed 137000/575757 combinations\n",
            "    Processed 138000/575757 combinations\n",
            "    Processed 139000/575757 combinations\n",
            "    Processed 140000/575757 combinations\n",
            "    Processed 141000/575757 combinations\n",
            "    Processed 142000/575757 combinations\n",
            "    Processed 143000/575757 combinations\n",
            "    Processed 144000/575757 combinations\n",
            "    Processed 145000/575757 combinations\n",
            "    Processed 146000/575757 combinations\n",
            "    Processed 147000/575757 combinations\n",
            "    Processed 148000/575757 combinations\n",
            "    Processed 149000/575757 combinations\n",
            "    Processed 150000/575757 combinations\n",
            "    Processed 151000/575757 combinations\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Run best subset selection (limit to 8 features for computational feasibility)\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== BEST SUBSET SELECTION ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m best_models = \u001b[43mbest_subset_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mbest_subset_selection\u001b[39m\u001b[34m(X, y, max_features)\u001b[39m\n\u001b[32m     59\u001b[39m selected_features = [feature_names[j] \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m feature_indices]\n\u001b[32m     60\u001b[39m X_subset = X[selected_features]\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m model_info = \u001b[43mfit_logistic_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Track best AIC model\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_info[\u001b[33m'\u001b[39m\u001b[33maic\u001b[39m\u001b[33m'\u001b[39m] < best_aic:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mfit_logistic_model\u001b[39m\u001b[34m(X_subset, y, feature_names)\u001b[39m\n\u001b[32m      5\u001b[39m X_with_const = sm.add_constant(X_subset)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mLogit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_with_const\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m result = model.fit(disp=\u001b[38;5;28;01mFalse\u001b[39;00m, maxiter=\u001b[32m1000\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maic\u001b[39m\u001b[33m'\u001b[39m: result.aic,\n\u001b[32m     13\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbic\u001b[39m\u001b[33m'\u001b[39m: result.bic,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m'\u001b[39m: result\n\u001b[32m     19\u001b[39m }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:475\u001b[39m, in \u001b[36mBinaryModel.__init__\u001b[39m\u001b[34m(self, endog, exog, offset, check_rank, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, offset=\u001b[38;5;28;01mNone\u001b[39;00m, check_rank=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m    473\u001b[39m     \u001b[38;5;66;03m# unconditional check, requires no extra kwargs added by subclasses\u001b[39;00m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_kwargs(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_rank\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m, MultinomialModel):\n\u001b[32m    478\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np.all((\u001b[38;5;28mself\u001b[39m.endog >= \u001b[32m0\u001b[39m) & (\u001b[38;5;28mself\u001b[39m.endog <= \u001b[32m1\u001b[39m)):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:185\u001b[39m, in \u001b[36mDiscreteModel.__init__\u001b[39m\u001b[34m(self, endog, exog, check_rank, **kwargs)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, check_rank=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs):\n\u001b[32m    184\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_rank = check_rank\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28mself\u001b[39m.raise_on_perfect_prediction = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# keep for backwards compat\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;28mself\u001b[39m.k_extra = \u001b[32m0\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/base/model.py:270\u001b[39m, in \u001b[36mLikelihoodModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.initialize()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/base/model.py:95\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, endog, exog, **kwargs)\u001b[39m\n\u001b[32m     93\u001b[39m missing = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mmissing\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     94\u001b[39m hasconst = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mhasconst\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                              \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[38;5;28mself\u001b[39m.data.k_constant\n\u001b[32m     98\u001b[39m \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28mself\u001b[39m.data.exog\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/base/model.py:135\u001b[39m, in \u001b[36mModel._handle_data\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog, missing, hasconst, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     data = \u001b[43mhandle_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# kwargs arrays could have changed, easier to just attach here\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/base/data.py:675\u001b[39m, in \u001b[36mhandle_data\u001b[39m\u001b[34m(endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m     exog = np.asarray(exog)\n\u001b[32m    674\u001b[39m klass = handle_data_class_factory(endog, exog)\n\u001b[32m--> \u001b[39m\u001b[32m675\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhasconst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m             \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/base/data.py:84\u001b[39m, in \u001b[36mModelData.__init__\u001b[39m\u001b[34m(self, endog, exog, missing, hasconst, **kwargs)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_endog = endog\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m.orig_exog = exog\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28mself\u001b[39m.endog, \u001b[38;5;28mself\u001b[39m.exog = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_endog_exog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28mself\u001b[39m.const_idx = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28mself\u001b[39m.k_constant = \u001b[32m0\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/statsmodels/base/data.py:507\u001b[39m, in \u001b[36mPandasData._convert_endog_exog\u001b[39m\u001b[34m(self, endog, exog)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_convert_endog_exog\u001b[39m(\u001b[38;5;28mself\u001b[39m, endog, exog=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    505\u001b[39m     \u001b[38;5;66;03m#TODO: remove this when we handle dtype systematically\u001b[39;00m\n\u001b[32m    506\u001b[39m     endog = np.asarray(endog)\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     exog = exog \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m endog.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exog.dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    509\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPandas data cast to numpy dtype of object. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33mCheck input data with np.asarray(data).\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/pandas/core/generic.py:2165\u001b[39m, in \u001b[36mNDFrame.__array__\u001b[39m\u001b[34m(self, dtype, copy)\u001b[39m\n\u001b[32m   2152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mgr.is_single_block \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.empty:\n\u001b[32m   2153\u001b[39m     \u001b[38;5;66;03m# check this manually, otherwise ._values will already return a copy\u001b[39;00m\n\u001b[32m   2154\u001b[39m     \u001b[38;5;66;03m# and np.array(values, copy=False) will not raise a warning\u001b[39;00m\n\u001b[32m   2155\u001b[39m     warnings.warn(\n\u001b[32m   2156\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mStarting with NumPy 2.0, the behavior of the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m'\u001b[39m\u001b[33m keyword has \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchanged and passing \u001b[39m\u001b[33m'\u001b[39m\u001b[33mcopy=False\u001b[39m\u001b[33m'\u001b[39m\u001b[33m raises an error when returning \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2163\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m   2164\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2165\u001b[39m values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\n\u001b[32m   2166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2167\u001b[39m     \u001b[38;5;66;03m# Note: branch avoids `copy=None` for NumPy 1.x support\u001b[39;00m\n\u001b[32m   2168\u001b[39m     arr = np.asarray(values, dtype=dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/pandas/core/frame.py:1127\u001b[39m, in \u001b[36mDataFrame._values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1125\u001b[39m blocks = mgr.blocks\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(blocks) != \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ensure_wrapped_if_datetimelike(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m)\n\u001b[32m   1129\u001b[39m arr = blocks[\u001b[32m0\u001b[39m].values\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m arr.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m   1131\u001b[39m     \u001b[38;5;66;03m# non-2D ExtensionArray\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/pandas/core/frame.py:12671\u001b[39m, in \u001b[36mDataFrame.values\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m  12597\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m  12598\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> np.ndarray:\n\u001b[32m  12599\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m  12600\u001b[39m \u001b[33;03m    Return a Numpy representation of the DataFrame.\u001b[39;00m\n\u001b[32m  12601\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m  12669\u001b[39m \u001b[33;03m           ['monkey', nan, None]], dtype=object)\u001b[39;00m\n\u001b[32m  12670\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m12671\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/pandas/core/internals/managers.py:1694\u001b[39m, in \u001b[36mBlockManager.as_array\u001b[39m\u001b[34m(self, dtype, copy, na_value)\u001b[39m\n\u001b[32m   1692\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m     arr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[32m   1696\u001b[39m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/jupyter-env/lib/python3.13/site-packages/pandas/core/internals/managers.py:1737\u001b[39m, in \u001b[36mBlockManager._interleave\u001b[39m\u001b[34m(self, dtype, na_value)\u001b[39m\n\u001b[32m   1735\u001b[39m         arr = blk.get_values(dtype)\n\u001b[32m   1736\u001b[39m         result[rl.indexer] = arr\n\u001b[32m-> \u001b[39m\u001b[32m1737\u001b[39m         itemmask[rl.indexer] = \u001b[32m1\u001b[39m\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m   1740\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def fit_logistic_model(X_subset, y, feature_names=None):\n",
        "    \"\"\"Fit logistic regression and return AIC, BIC, and model info\"\"\"\n",
        "    try:\n",
        "        # Add constant\n",
        "        X_with_const = sm.add_constant(X_subset)\n",
        "        \n",
        "        # Fit model\n",
        "        model = Logit(y, X_with_const)\n",
        "        result = model.fit(disp=False, maxiter=1000)\n",
        "        \n",
        "        return {\n",
        "            'aic': result.aic,\n",
        "            'bic': result.bic,\n",
        "            'll': result.llf,\n",
        "            'n_params': len(result.params),\n",
        "            'converged': result.mle_retvals['converged'],\n",
        "            'features': feature_names if feature_names else list(range(X_subset.shape[1])),\n",
        "            'result': result\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'aic': np.inf,\n",
        "            'bic': np.inf, \n",
        "            'll': -np.inf,\n",
        "            'n_params': X_subset.shape[1] + 1,\n",
        "            'converged': False,\n",
        "            'features': feature_names if feature_names else list(range(X_subset.shape[1])),\n",
        "            'result': None,\n",
        "            'error': str(e)\n",
        "        }\n",
        "\n",
        "def best_subset_selection(X, y, max_features=10):\n",
        "    \"\"\"Perform best subset selection up to max_features\"\"\"\n",
        "    feature_names = X.columns.tolist()\n",
        "    n_features = len(feature_names)\n",
        "    \n",
        "    print(f\"Starting best subset selection with {n_features} features\")\n",
        "    print(f\"Evaluating subsets up to size {min(max_features, n_features)}\")\n",
        "    \n",
        "    best_models = {}\n",
        "    \n",
        "    # Try subsets of increasing size\n",
        "    for k in range(1, min(max_features, n_features) + 1):\n",
        "        print(f\"\\nEvaluating subsets of size {k}...\")\n",
        "        \n",
        "        best_aic = np.inf\n",
        "        best_bic = np.inf\n",
        "        best_model_aic = None\n",
        "        best_model_bic = None\n",
        "        \n",
        "        # Generate all combinations of k features\n",
        "        n_combinations = len(list(combinations(range(n_features), k)))\n",
        "        print(f\"  Testing {n_combinations} combinations\")\n",
        "        \n",
        "        for i, feature_indices in enumerate(combinations(range(n_features), k)):\n",
        "            if i > 0 and i % 1000 == 0:\n",
        "                print(f\"    Processed {i}/{n_combinations} combinations\")\n",
        "            \n",
        "            selected_features = [feature_names[j] for j in feature_indices]\n",
        "            X_subset = X[selected_features]\n",
        "            \n",
        "            model_info = fit_logistic_model(X_subset, y, selected_features)\n",
        "            \n",
        "            # Track best AIC model\n",
        "            if model_info['aic'] < best_aic:\n",
        "                best_aic = model_info['aic']\n",
        "                best_model_aic = model_info.copy()\n",
        "            \n",
        "            # Track best BIC model\n",
        "            if model_info['bic'] < best_bic:\n",
        "                best_bic = model_info['bic']\n",
        "                best_model_bic = model_info.copy()\n",
        "        \n",
        "        best_models[k] = {\n",
        "            'aic_model': best_model_aic,\n",
        "            'bic_model': best_model_bic\n",
        "        }\n",
        "        \n",
        "        print(f\"  Best AIC ({k} features): {best_aic:.3f}, Features: {best_model_aic['features']}\")\n",
        "        print(f\"  Best BIC ({k} features): {best_bic:.3f}, Features: {best_model_bic['features']}\")\n",
        "    \n",
        "    return best_models\n",
        "\n",
        "# Run best subset selection (limit to 8 features for computational feasibility)\n",
        "print(\"=== BEST SUBSET SELECTION ===\")\n",
        "best_models = best_subset_selection(X_train_scaled, y_train, max_features=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Method 2: Forward Stepwise Selection\n",
        "\n",
        "**Approach**: Start with no variables, add one at a time\n",
        "**Selection**: At each step, add the variable that most improves AIC/BIC\n",
        "**Stopping**: When no variable improves the criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def forward_selection(X, y, criterion='aic', max_features=None):\n",
        "    \"\"\"Forward stepwise selection based on AIC or BIC\"\"\"\n",
        "    feature_names = X.columns.tolist()\n",
        "    n_features = len(feature_names)\n",
        "    max_features = max_features or n_features\n",
        "    \n",
        "    selected_features = []\n",
        "    remaining_features = feature_names.copy()\n",
        "    \n",
        "    selection_history = []\n",
        "    \n",
        "    print(f\"\\n=== FORWARD SELECTION (criterion: {criterion.upper()}) ===\")\n",
        "    \n",
        "    # Start with intercept-only model\n",
        "    baseline_model = fit_logistic_model(pd.DataFrame(np.ones(len(y))), y, ['intercept_only'])\n",
        "    current_score = baseline_model[criterion]\n",
        "    \n",
        "    print(f\"Baseline (intercept-only) {criterion.upper()}: {current_score:.3f}\")\n",
        "    \n",
        "    step = 0\n",
        "    while remaining_features and len(selected_features) < max_features:\n",
        "        step += 1\n",
        "        print(f\"\\nStep {step}: Testing {len(remaining_features)} candidates\")\n",
        "        \n",
        "        best_score = current_score\n",
        "        best_feature = None\n",
        "        \n",
        "        # Try adding each remaining feature\n",
        "        for feature in remaining_features:\n",
        "            test_features = selected_features + [feature]\n",
        "            X_subset = X[test_features]\n",
        "            \n",
        "            model_info = fit_logistic_model(X_subset, y, test_features)\n",
        "            score = model_info[criterion]\n",
        "            \n",
        "            if score < best_score:  # Lower AIC/BIC is better\n",
        "                best_score = score\n",
        "                best_feature = feature\n",
        "        \n",
        "        # Check if we found an improvement\n",
        "        if best_feature is not None:\n",
        "            selected_features.append(best_feature)\n",
        "            remaining_features.remove(best_feature)\n",
        "            current_score = best_score\n",
        "            \n",
        "            print(f\"  Added: {best_feature}\")\n",
        "            print(f\"  New {criterion.upper()}: {current_score:.3f}\")\n",
        "            print(f\"  Selected features: {selected_features}\")\n",
        "            \n",
        "            selection_history.append({\n",
        "                'step': step,\n",
        "                'added_feature': best_feature,\n",
        "                'features': selected_features.copy(),\n",
        "                'score': current_score\n",
        "            })\n",
        "        else:\n",
        "            print(f\"  No improvement found. Stopping.\")\n",
        "            break\n",
        "    \n",
        "    # Fit final model\n",
        "    if selected_features:\n",
        "        final_model = fit_logistic_model(X[selected_features], y, selected_features)\n",
        "    else:\n",
        "        final_model = baseline_model\n",
        "    \n",
        "    return {\n",
        "        'selected_features': selected_features,\n",
        "        'final_model': final_model,\n",
        "        'selection_history': selection_history,\n",
        "        'criterion': criterion\n",
        "    }\n",
        "\n",
        "# Run forward selection with both AIC and BIC\n",
        "forward_aic = forward_selection(X_train_scaled, y_train, criterion='aic', max_features=15)\n",
        "forward_bic = forward_selection(X_train_scaled, y_train, criterion='bic', max_features=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Method 3: Backward Stepwise Selection\n",
        "\n",
        "**Approach**: Start with all variables, remove one at a time\n",
        "**Selection**: At each step, remove the variable that most improves AIC/BIC\n",
        "**Stopping**: When removing any variable worsens the criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def backward_elimination(X, y, criterion='aic', min_features=1):\n",
        "    \"\"\"Backward elimination based on AIC or BIC\"\"\"\n",
        "    feature_names = X.columns.tolist()\n",
        "    selected_features = feature_names.copy()\n",
        "    \n",
        "    elimination_history = []\n",
        "    \n",
        "    print(f\"\\n=== BACKWARD ELIMINATION (criterion: {criterion.upper()}) ===\")\n",
        "    \n",
        "    # Start with full model\n",
        "    current_model = fit_logistic_model(X[selected_features], y, selected_features)\n",
        "    current_score = current_model[criterion]\n",
        "    \n",
        "    print(f\"Starting with {len(selected_features)} features\")\n",
        "    print(f\"Initial {criterion.upper()}: {current_score:.3f}\")\n",
        "    \n",
        "    step = 0\n",
        "    while len(selected_features) > min_features:\n",
        "        step += 1\n",
        "        print(f\"\\nStep {step}: Testing removal of {len(selected_features)} features\")\n",
        "        \n",
        "        best_score = current_score\n",
        "        worst_feature = None\n",
        "        \n",
        "        # Try removing each feature\n",
        "        for feature in selected_features:\n",
        "            test_features = [f for f in selected_features if f != feature]\n",
        "            if len(test_features) == 0:  # Don't remove last feature\n",
        "                continue\n",
        "                \n",
        "            X_subset = X[test_features]\n",
        "            model_info = fit_logistic_model(X_subset, y, test_features)\n",
        "            score = model_info[criterion]\n",
        "            \n",
        "            if score < best_score:  # Lower AIC/BIC is better\n",
        "                best_score = score\n",
        "                worst_feature = feature\n",
        "        \n",
        "        # Check if we found an improvement\n",
        "        if worst_feature is not None:\n",
        "            selected_features.remove(worst_feature)\n",
        "            current_score = best_score\n",
        "            \n",
        "            print(f\"  Removed: {worst_feature}\")\n",
        "            print(f\"  New {criterion.upper()}: {current_score:.3f}\")\n",
        "            print(f\"  Remaining features ({len(selected_features)}): {selected_features}\")\n",
        "            \n",
        "            elimination_history.append({\n",
        "                'step': step,\n",
        "                'removed_feature': worst_feature,\n",
        "                'features': selected_features.copy(),\n",
        "                'score': current_score\n",
        "            })\n",
        "        else:\n",
        "            print(f\"  No improvement found. Stopping.\")\n",
        "            break\n",
        "    \n",
        "    # Fit final model\n",
        "    final_model = fit_logistic_model(X[selected_features], y, selected_features)\n",
        "    \n",
        "    return {\n",
        "        'selected_features': selected_features,\n",
        "        'final_model': final_model,\n",
        "        'elimination_history': elimination_history,\n",
        "        'criterion': criterion\n",
        "    }\n",
        "\n",
        "# Run backward elimination with both AIC and BIC\n",
        "backward_aic = backward_elimination(X_train_scaled, y_train, criterion='aic', min_features=3)\n",
        "backward_bic = backward_elimination(X_train_scaled, y_train, criterion='bic', min_features=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Model Comparison and Selection\n",
        "\n",
        "**Strategy**: Compare all methods and select the winner based on:\n",
        "1. **Statistical Criteria**: AIC and BIC scores\n",
        "2. **Validation Performance**: 10-fold cross-validation AUC\n",
        "3. **Interpretability**: Number of features and practical significance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Collect all candidate models\n",
        "candidate_models = []\n",
        "\n",
        "# Add best subset models\n",
        "print(\"=== COLLECTING CANDIDATE MODELS ===\")\n",
        "for k, models in best_models.items():\n",
        "    # AIC winner for this subset size\n",
        "    if models['aic_model']['converged']:\n",
        "        candidate_models.append({\n",
        "            'method': f'Best Subset (k={k})',\n",
        "            'criterion_used': 'AIC',\n",
        "            'features': models['aic_model']['features'],\n",
        "            'n_features': len(models['aic_model']['features']),\n",
        "            'aic': models['aic_model']['aic'],\n",
        "            'bic': models['aic_model']['bic'],\n",
        "            'model_info': models['aic_model']\n",
        "        })\n",
        "    \n",
        "    # BIC winner for this subset size\n",
        "    if models['bic_model']['converged']:\n",
        "        candidate_models.append({\n",
        "            'method': f'Best Subset (k={k})',\n",
        "            'criterion_used': 'BIC',\n",
        "            'features': models['bic_model']['features'],\n",
        "            'n_features': len(models['bic_model']['features']),\n",
        "            'aic': models['bic_model']['aic'],\n",
        "            'bic': models['bic_model']['bic'],\n",
        "            'model_info': models['bic_model']\n",
        "        })\n",
        "\n",
        "# Add forward selection models\n",
        "if forward_aic['final_model']['converged']:\n",
        "    candidate_models.append({\n",
        "        'method': 'Forward Selection',\n",
        "        'criterion_used': 'AIC',\n",
        "        'features': forward_aic['selected_features'],\n",
        "        'n_features': len(forward_aic['selected_features']),\n",
        "        'aic': forward_aic['final_model']['aic'],\n",
        "        'bic': forward_aic['final_model']['bic'],\n",
        "        'model_info': forward_aic['final_model']\n",
        "    })\n",
        "\n",
        "if forward_bic['final_model']['converged']:\n",
        "    candidate_models.append({\n",
        "        'method': 'Forward Selection',\n",
        "        'criterion_used': 'BIC',\n",
        "        'features': forward_bic['selected_features'],\n",
        "        'n_features': len(forward_bic['selected_features']),\n",
        "        'aic': forward_bic['final_model']['aic'],\n",
        "        'bic': forward_bic['final_model']['bic'],\n",
        "        'model_info': forward_bic['final_model']\n",
        "    })\n",
        "\n",
        "# Add backward elimination models\n",
        "if backward_aic['final_model']['converged']:\n",
        "    candidate_models.append({\n",
        "        'method': 'Backward Elimination',\n",
        "        'criterion_used': 'AIC',\n",
        "        'features': backward_aic['selected_features'],\n",
        "        'n_features': len(backward_aic['selected_features']),\n",
        "        'aic': backward_aic['final_model']['aic'],\n",
        "        'bic': backward_aic['final_model']['bic'],\n",
        "        'model_info': backward_aic['final_model']\n",
        "    })\n",
        "\n",
        "if backward_bic['final_model']['converged']:\n",
        "    candidate_models.append({\n",
        "        'method': 'Backward Elimination',\n",
        "        'criterion_used': 'BIC',\n",
        "        'features': backward_bic['selected_features'],\n",
        "        'n_features': len(backward_bic['selected_features']),\n",
        "        'aic': backward_bic['final_model']['aic'],\n",
        "        'bic': backward_bic['final_model']['bic'],\n",
        "        'model_info': backward_bic['final_model']\n",
        "    })\n",
        "\n",
        "print(f\"Collected {len(candidate_models)} candidate models\")\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_df = pd.DataFrame([{\n",
        "    'Method': model['method'],\n",
        "    'Criterion': model['criterion_used'],\n",
        "    'N_Features': model['n_features'],\n",
        "    'AIC': model['aic'],\n",
        "    'BIC': model['bic'],\n",
        "    'Features': ', '.join(model['features'][:3]) + ('...' if len(model['features']) > 3 else '')\n",
        "} for model in candidate_models])\n",
        "\n",
        "# Sort by AIC\n",
        "comparison_df = comparison_df.sort_values('AIC').reset_index(drop=True)\n",
        "print(\"\\n=== MODEL COMPARISON (sorted by AIC) ===\")\n",
        "print(comparison_df.to_string(index=False, float_format='%.3f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def cross_validate_model(features, X, y, cv_folds=10):\n",
        "    \"\"\"Perform cross-validation for a feature set\"\"\"\n",
        "    from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import roc_auc_score, make_scorer\n",
        "    \n",
        "    # Prepare data\n",
        "    X_subset = X[features]\n",
        "    \n",
        "    # Use sklearn for reliable cross-validation\n",
        "    lr = LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')\n",
        "    \n",
        "    # Stratified K-fold to maintain class balance\n",
        "    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "    \n",
        "    # Calculate AUC scores\n",
        "    auc_scores = cross_val_score(lr, X_subset, y, cv=cv, scoring='roc_auc')\n",
        "    \n",
        "    return {\n",
        "        'mean_auc': auc_scores.mean(),\n",
        "        'std_auc': auc_scores.std(),\n",
        "        'auc_scores': auc_scores\n",
        "    }\n",
        "\n",
        "# Cross-validate top models (by AIC)\n",
        "print(\"\\n=== CROSS-VALIDATION OF TOP MODELS ===\")\n",
        "\n",
        "# Select top 5 models by AIC for CV\n",
        "top_models = candidate_models[:5]\n",
        "\n",
        "cv_results = []\n",
        "for i, model in enumerate(top_models):\n",
        "    print(f\"\\nCV for Model {i+1}: {model['method']} ({model['criterion_used']})\")\n",
        "    print(f\"Features ({model['n_features']}): {model['features']}\")\n",
        "    \n",
        "    cv_result = cross_validate_model(model['features'], X_train_scaled, y_train)\n",
        "    \n",
        "    cv_results.append({\n",
        "        'model_index': i,\n",
        "        'method': model['method'],\n",
        "        'criterion': model['criterion_used'],\n",
        "        'n_features': model['n_features'],\n",
        "        'aic': model['aic'],\n",
        "        'bic': model['bic'],\n",
        "        'cv_auc_mean': cv_result['mean_auc'],\n",
        "        'cv_auc_std': cv_result['std_auc'],\n",
        "        'features': model['features']\n",
        "    })\n",
        "    \n",
        "    print(f\"CV AUC: {cv_result['mean_auc']:.4f} (±{cv_result['std_auc']:.4f})\")\n",
        "\n",
        "# Create final comparison\n",
        "final_comparison = pd.DataFrame(cv_results)\n",
        "print(\"\\n=== FINAL MODEL COMPARISON ===\")\n",
        "print(final_comparison[['method', 'criterion', 'n_features', 'aic', 'bic', 'cv_auc_mean', 'cv_auc_std']].to_string(index=False, float_format='%.4f'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## Winner Selection and Final Model\n",
        "\n",
        "**Selection Criteria**:\n",
        "1. **Primary**: Lowest AIC (penalizes complexity less than BIC)\n",
        "2. **Validation**: Competitive cross-validation AUC\n",
        "3. **Practical**: Reasonable number of features for interpretation\n",
        "\n",
        "**Final Model**: The winner will be our minimal useful predictor set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Select the winner\n",
        "winner_idx = final_comparison['aic'].idxmin()\n",
        "winner = final_comparison.loc[winner_idx]\n",
        "\n",
        "print(\"=== WINNER SELECTION ===\")\n",
        "print(f\"Method: {winner['method']} ({winner['criterion']})\")\n",
        "print(f\"Number of features: {winner['n_features']}\")\n",
        "print(f\"AIC: {winner['aic']:.3f}\")\n",
        "print(f\"BIC: {winner['bic']:.3f}\")\n",
        "print(f\"Cross-validation AUC: {winner['cv_auc_mean']:.4f} (±{winner['cv_auc_std']:.4f})\")\n",
        "print(f\"\\nSelected features: {winner['features']}\")\n",
        "\n",
        "# Fit final model on full training set\n",
        "final_features = winner['features']\n",
        "X_final = X_train_scaled[final_features]\n",
        "X_final_with_const = sm.add_constant(X_final)\n",
        "\n",
        "final_model = Logit(y_train, X_final_with_const).fit()\n",
        "\n",
        "print(\"\\n=== FINAL MODEL SUMMARY ===\")\n",
        "print(final_model.summary())\n",
        "\n",
        "# Test set performance\n",
        "X_test_final = X_test_scaled[final_features]\n",
        "X_test_final_with_const = sm.add_constant(X_test_final)\n",
        "\n",
        "# Predictions\n",
        "test_probs = final_model.predict(X_test_final_with_const)\n",
        "test_preds = (test_probs > 0.5).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_preds)\n",
        "test_precision = precision_score(y_test, test_preds)\n",
        "test_recall = recall_score(y_test, test_preds)\n",
        "test_f1 = f1_score(y_test, test_preds)\n",
        "test_auc = roc_auc_score(y_test, test_probs)\n",
        "\n",
        "print(\"\\n=== TEST SET PERFORMANCE ===\")\n",
        "print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall:    {test_recall:.4f}\")\n",
        "print(f\"F1-Score:  {test_f1:.4f}\")\n",
        "print(f\"AUC:       {test_auc:.4f}\")\n",
        "\n",
        "# Feature importance (coefficients)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': final_features,\n",
        "    'coefficient': final_model.params[1:],  # Exclude intercept\n",
        "    'p_value': final_model.pvalues[1:],\n",
        "    'conf_low': final_model.conf_int()[0][1:],\n",
        "    'conf_high': final_model.conf_int()[1][1:]\n",
        "})\n",
        "feature_importance['odds_ratio'] = np.exp(feature_importance['coefficient'])\n",
        "feature_importance = feature_importance.sort_values('coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(\"\\n=== FEATURE IMPORTANCE (Minimal Useful Predictor Set) ===\")\n",
        "print(feature_importance.to_string(index=False, float_format='%.4f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Visualization of results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Model comparison (AIC vs BIC)\n",
        "ax1 = axes[0, 0]\n",
        "scatter = ax1.scatter(final_comparison['aic'], final_comparison['bic'], \n",
        "                     c=final_comparison['cv_auc_mean'], cmap='viridis', s=100)\n",
        "ax1.set_xlabel('AIC')\n",
        "ax1.set_ylabel('BIC')\n",
        "ax1.set_title('Model Selection: AIC vs BIC\\n(Color = CV AUC)')\n",
        "plt.colorbar(scatter, ax=ax1, label='CV AUC')\n",
        "\n",
        "# Mark winner\n",
        "winner_aic = winner['aic']\n",
        "winner_bic = winner['bic']\n",
        "ax1.scatter(winner_aic, winner_bic, color='red', s=200, marker='*', \n",
        "           edgecolor='black', linewidth=2, label='Winner')\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Feature importance\n",
        "ax2 = axes[0, 1]\n",
        "y_pos = np.arange(len(feature_importance))\n",
        "bars = ax2.barh(y_pos, feature_importance['coefficient'], \n",
        "                color=['red' if x < 0 else 'blue' for x in feature_importance['coefficient']])\n",
        "ax2.set_yticks(y_pos)\n",
        "ax2.set_yticklabels(feature_importance['feature'])\n",
        "ax2.set_xlabel('Coefficient')\n",
        "ax2.set_title('Feature Coefficients (Minimal Set)')\n",
        "ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "\n",
        "# 3. ROC Curve\n",
        "from sklearn.metrics import roc_curve\n",
        "ax3 = axes[1, 0]\n",
        "fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
        "ax3.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {test_auc:.3f})')\n",
        "ax3.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='Random')\n",
        "ax3.set_xlim([0.0, 1.0])\n",
        "ax3.set_ylim([0.0, 1.05])\n",
        "ax3.set_xlabel('False Positive Rate')\n",
        "ax3.set_ylabel('True Positive Rate')\n",
        "ax3.set_title('ROC Curve - Final Model')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "ax4 = axes[1, 1]\n",
        "cm = confusion_matrix(y_test, test_preds)\n",
        "im = ax4.imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "ax4.set_title('Confusion Matrix')\n",
        "ax4.set_xlabel('Predicted Label')\n",
        "ax4.set_ylabel('True Label')\n",
        "\n",
        "# Add text annotations\n",
        "thresh = cm.max() / 2.\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax4.text(j, i, format(cm[i, j], 'd'),\n",
        "                ha=\"center\", va=\"center\",\n",
        "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUBSET SELECTION COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best method: {winner['method']} using {winner['criterion']}\")\n",
        "print(f\"Minimal useful predictor set: {len(final_features)} features\")\n",
        "print(f\"Features: {final_features}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
